# ðŸ”„ Deduplication Logic - Complete Explanation

## Overview

The upload system handles duplicates at **3 levels** to ensure data integrity.

---

## ðŸ“Š The 3 Levels of Deduplication

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        LEVEL 1: GCS FILES                       â”‚
â”‚  Multiple parquet files may contain same paper                  â”‚
â”‚                                                                  â”‚
â”‚  processed_v2/                                                   â”‚
â”‚  â”œâ”€â”€ processed_AI_1234567890.parquet     â†’ Paper X, Y, Z        â”‚
â”‚  â”œâ”€â”€ processed_Healthcare_1234567891.parquet â†’ Paper Y, Z, W    â”‚
â”‚  â””â”€â”€ processed_Quantum_1234567892.parquet    â†’ Paper Z, Q       â”‚
â”‚                                                                  â”‚
â”‚  ACTION: df.drop_duplicates(subset='paperId', keep='first')     â”‚
â”‚  RESULT: Paper X, Y, Z, W, Q (unique)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   LEVEL 2: SUPABASE CHECK                       â”‚
â”‚  Before uploading, check what's already in database             â”‚
â”‚                                                                  â”‚
â”‚  Query: SELECT paper_id FROM papers                             â”‚
â”‚  Result: {Paper X, Paper Y} already exist                       â”‚
â”‚                                                                  â”‚
â”‚  ACTION: if paper_id in existing_ids: skip()                    â”‚
â”‚  RESULT: Only upload Paper Z, W, Q                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LEVEL 3: SQL CONSTRAINT                      â”‚
â”‚  Safety net: even if duplicate slips through                    â”‚
â”‚                                                                  â”‚
â”‚  INSERT INTO papers (paper_id, ...)                             â”‚
â”‚  VALUES ('paper_z', ...)                                        â”‚
â”‚  ON CONFLICT (paper_id) DO NOTHING                              â”‚
â”‚                                                                  â”‚
â”‚  ACTION: Database rejects duplicates automatically              â”‚
â”‚  RESULT: Only unique papers inserted                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸŽ¯ Example Scenario

### **Setup:**
```
GCS Files:
â”œâ”€â”€ processed_AI_12345.parquet
â”‚   â””â”€â”€ Papers: A, B, C, D
â”‚
â”œâ”€â”€ processed_AI_67890.parquet (from mitigation)
â”‚   â””â”€â”€ Papers: C, D, E, F
â”‚
â””â”€â”€ processed_Healthcare_11111.parquet
    â””â”€â”€ Papers: D, E, G, H

Supabase Database (already has):
â””â”€â”€ Papers: A, B, D, G
```

### **Upload Process:**

#### **Step 1: Load from GCS**
```python
Load all 3 files:
Total papers loaded: 12
  - A, B, C, D (from file 1)
  - C, D, E, F (from file 2)
  - D, E, G, H (from file 3)
```

#### **Step 2: Level 1 Deduplication (GCS)**
```python
df_all = pd.concat([file1, file2, file3])
df_all = df_all.drop_duplicates(subset='paperId', keep='first')

Duplicates within GCS:
  - Paper C: appears 2 times â†’ Keep first
  - Paper D: appears 3 times â†’ Keep first
  - Paper E: appears 2 times â†’ Keep first

Result: A, B, C, D, E, F, G, H (8 unique papers)
Removed: 4 duplicates
```

#### **Step 3: Level 2 Deduplication (Supabase Check)**
```python
existing_ids = {A, B, D, G}  # From database

For each paper:
  - A: in existing_ids â†’ SKIP
  - B: in existing_ids â†’ SKIP
  - C: NOT in existing_ids â†’ UPLOAD
  - D: in existing_ids â†’ SKIP
  - E: NOT in existing_ids â†’ UPLOAD
  - F: NOT in existing_ids â†’ UPLOAD
  - G: in existing_ids â†’ SKIP
  - H: NOT in existing_ids â†’ UPLOAD

Papers to upload: C, E, F, H (4 papers)
Skipped: A, B, D, G (4 papers already in DB)
```

#### **Step 4: Level 3 Deduplication (SQL)**
```sql
INSERT INTO papers (paper_id, ...) VALUES ('C', ...)
ON CONFLICT (paper_id) DO NOTHING;  -- Paper C inserted

INSERT INTO papers (paper_id, ...) VALUES ('E', ...)
ON CONFLICT (paper_id) DO NOTHING;  -- Paper E inserted

INSERT INTO papers (paper_id, ...) VALUES ('F', ...)
ON CONFLICT (paper_id) DO NOTHING;  -- Paper F inserted

INSERT INTO papers (paper_id, ...) VALUES ('H', ...)
ON CONFLICT (paper_id) DO NOTHING;  -- Paper H inserted
```

### **Final Result:**
```
Supabase Database now has:
â””â”€â”€ Papers: A, B, C, D, E, F, G, H (8 total)

Upload Statistics:
â”œâ”€â”€ Papers in GCS (after dedupe): 8
â”œâ”€â”€ Inserted: 4 (C, E, F, H)
â”œâ”€â”€ Skipped: 4 (A, B, D, G)
â””â”€â”€ Failed: 0
```

---

## ðŸ”¬ Testing Deduplication

### **Test Script:**
```bash
# Run test with 10 papers
python test_supabase_upload.py
```

### **What the test does:**

```
1. First Upload (10 papers):
   â”œâ”€â”€ Load 10 papers from GCS
   â”œâ”€â”€ Check Supabase for existing papers
   â”œâ”€â”€ Upload new papers
   â””â”€â”€ Result: Some inserted, some skipped

2. Second Upload (same 10 papers):
   â”œâ”€â”€ Load same 10 papers from GCS
   â”œâ”€â”€ Check Supabase (now includes first upload)
   â”œâ”€â”€ All 10 papers already exist
   â””â”€â”€ Result: 0 inserted, 10 skipped âœ…

Expected Output:
  First run: Inserted: X, Skipped: Y
  Second run: Inserted: 0, Skipped: 10
  â†’ Deduplication working correctly!
```

---

## ðŸ“‹ Code Walkthrough

### **Level 1: GCS Deduplication**
```python
# File: upload_papers_to_supabase.py, Line ~236

# Load all parquet files
dataframes = self.load_papers_from_gcs()

# Combine into single DataFrame
df_all = pd.concat(dataframes, ignore_index=True)

# Deduplicate by paperId
original_count = len(df_all)
df_all = df_all.drop_duplicates(subset='paperId', keep='first')
dedupe_count = original_count - len(df_all)

if dedupe_count > 0:
    logger.info(f"ðŸ”„ Removed {dedupe_count} duplicate papers")
```

### **Level 2: Supabase Check**
```python
# File: upload_papers_to_supabase.py, Line ~41

async def get_existing_paper_ids(self, conn) -> set:
    """Get set of paper IDs already in database"""
    rows = await conn.fetch("SELECT paper_id FROM papers")
    existing_ids = {row['paper_id'] for row in rows}
    return existing_ids

# Then in upload_papers_batch(), Line ~177:
for paper in papers:
    paper_id = paper.get('paper_id')
    
    if paper_id in existing_ids:
        stats['skipped'] += 1
        continue  # Skip this paper
    
    # Only upload if not in existing_ids
    await conn.execute(insert_query, *values)
```

### **Level 3: SQL Constraint**
```python
# File: upload_papers_to_supabase.py, Line ~190

query = f"""
    INSERT INTO papers ({', '.join(columns)})
    VALUES ({', '.join(placeholders)})
    ON CONFLICT (paper_id) DO NOTHING
"""

await conn.execute(query, *values)
```

**SQL Constraint in Supabase:**
```sql
-- The papers table has:
paper_id TEXT UNIQUE NOT NULL

-- This creates a UNIQUE constraint
-- ON CONFLICT (paper_id) DO NOTHING means:
-- "If paper_id already exists, silently ignore this insert"
```

---

## ðŸŽ¯ Why 3 Levels?

### **Level 1 (GCS) - Performance**
- **Why:** Reduces data to upload
- **Example:** 10,000 papers across 5 files â†’ 8,000 unique
- **Benefit:** 20% less data to process, faster upload

### **Level 2 (Supabase Check) - Efficiency**
- **Why:** Avoid unnecessary INSERT queries
- **Example:** 8,000 unique papers, 6,000 already in DB â†’ only upload 2,000
- **Benefit:** 75% fewer database operations

### **Level 3 (SQL Constraint) - Safety**
- **Why:** Guarantee data integrity even if logic fails
- **Example:** Race condition, concurrent uploads
- **Benefit:** Database ensures no duplicates ever exist

---

## ðŸ§ª Test Outputs

### **Expected Output (First Run):**
```
================================================================================
ðŸš€ STARTING PAPER UPLOAD TO SUPABASE
================================================================================

ðŸ“¥ Loading papers from GCS...
ðŸ“¦ Found 3 parquet files
  âœ… Loaded 500 papers from processed_AI_*.parquet
  âœ… Loaded 450 papers from processed_Healthcare_*.parquet
  âœ… Loaded 380 papers from processed_Quantum_*.parquet

ðŸ”„ Removed 45 duplicate papers (Level 1)
ðŸ“Š Total unique papers in GCS: 1285

âš ï¸ Limited to first 10 papers (test mode)

ðŸ”Œ Connecting to Supabase...
ðŸ“Š Found 0 existing papers in database

ðŸ“¤ Uploading in batches of 10...
  Batch 1/1 (10 papers)...
    âœ… Inserted: 10, Skipped: 0, Failed: 0 (Level 2)

================================================================================
âœ… UPLOAD COMPLETE
================================================================================
ðŸ“Š Papers in GCS: 10
âœ… Papers inserted: 10
â­ï¸  Papers skipped: 0
âŒ Papers failed: 0
================================================================================

ðŸ”„ Running second upload to test deduplication...

ðŸ“Š Found 10 existing papers in database

ðŸ“¤ Uploading in batches of 10...
  Batch 1/1 (10 papers)...
    âœ… Inserted: 0, Skipped: 10, Failed: 0 (Level 2)

ðŸ“Š Second Upload Results:
   Inserted: 0
   Skipped: 10

âœ… DEDUPLICATION VERIFIED!
   â†’ All 10 papers were correctly identified as duplicates
   â†’ No papers were inserted on second run
```

---

## ðŸ” Debugging Deduplication

### **Check GCS Duplicates:**
```python
# In Python
import pandas as pd

df1 = pd.read_parquet('gs://bucket/processed_v2/file1.parquet')
df2 = pd.read_parquet('gs://bucket/processed_v2/file2.parquet')

# Find duplicates
df_all = pd.concat([df1, df2])
duplicates = df_all[df_all.duplicated(subset='paperId', keep=False)]
print(f"Duplicate papers: {len(duplicates)}")
print(duplicates[['paperId', 'title']])
```

### **Check Supabase:**
```sql
-- Count total papers
SELECT COUNT(*) FROM papers;

-- Find duplicates (should be 0!)
SELECT paper_id, COUNT(*) as count
FROM papers
GROUP BY paper_id
HAVING COUNT(*) > 1;

-- Check specific paper
SELECT * FROM papers WHERE paper_id = 'xyz123';
```

---

## âœ… Guarantees

With all 3 levels, the system guarantees:

1. âœ… **No duplicates within GCS upload batch**
2. âœ… **No duplicates between GCS and existing Supabase data**
3. âœ… **No duplicates in database even if logic fails**
4. âœ… **Concurrent uploads won't create duplicates**
5. âœ… **Safe to run upload multiple times**

---

## ðŸš€ Run the Test

```bash
# Make sure .env has Supabase credentials
python test_supabase_upload.py

# Expected:
# âœ… First run: Inserts 10 papers
# âœ… Second run (automatic): Skips all 10 papers
# âœ… Deduplication verified!
```

---

**Status:** âœ… Triple-redundant deduplication implemented and tested

